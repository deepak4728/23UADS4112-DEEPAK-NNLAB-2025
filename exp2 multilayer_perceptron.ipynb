{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Objective:**\n",
        "WAP to implement a multi-layer perceptron (MLP) network with one hidden layer using numpy in Python. Demonstrate that it can learn the XOR Boolean function."
      ],
      "metadata": {
        "id": "P9TdAVdDocQs"
      },
      "id": "P9TdAVdDocQs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Description of the Model:**\n",
        "- This is a Multi-Layer Perceptron (MLP) with a single hidden layer and a step activation function.\n",
        "- It is trained using backpropagation and attempts to learn the XOR function."
      ],
      "metadata": {
        "id": "9xAnXGzWORq-"
      },
      "id": "9xAnXGzWORq-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Python Implementation**"
      ],
      "metadata": {
        "id": "J6rizhYCMFji"
      },
      "id": "J6rizhYCMFji"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "17adaff8-23c0-4f38-92ac-965aa302f9aa",
      "metadata": {
        "id": "17adaff8-23c0-4f38-92ac-965aa302f9aa",
        "outputId": "d356d026-0755-477a-9984-16c9ae9993b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MLP for XOR\n",
            "Accuracy: 100.00%\n",
            "Confusion Matrix:\n",
            "[[2 0]\n",
            " [0 2]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "class MLP_XOR:\n",
        "    def __init__(self, input_size, hidden_size, lr=0.1, epochs=1000):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = 1  # Single output neuron for binary classification\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Xavier Initialization for stability\n",
        "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2.0 / self.input_size)\n",
        "        self.b1 = np.zeros((1, self.hidden_size))\n",
        "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2.0 / self.hidden_size)\n",
        "        self.b2 = np.zeros((1, self.output_size))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        error = y - output\n",
        "        d_output = error * self.sigmoid_derivative(output)\n",
        "        error_hidden = d_output.dot(self.W2.T)\n",
        "        d_hidden = error_hidden * self.relu_derivative(self.a1)\n",
        "\n",
        "        # Update weights and biases using learning rate\n",
        "        self.W2 += self.a1.T.dot(d_output) * self.lr\n",
        "        self.b2 += np.sum(d_output, axis=0, keepdims=True) * self.lr\n",
        "        self.W1 += X.T.dot(d_hidden) * self.lr\n",
        "        self.b1 += np.sum(d_hidden, axis=0, keepdims=True) * self.lr\n",
        "\n",
        "    def train(self, X, y):\n",
        "        for _ in range(self.epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.forward(X) > 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        correct = np.sum(predictions == y)\n",
        "        accuracy = (correct / len(y)) * 100\n",
        "        print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(y, predictions)\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(cm)\n",
        "\n",
        "# XOR Truth Table\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Train MLP on XOR\n",
        "print(\"Training MLP for XOR\")\n",
        "mlp_xor = MLP_XOR(input_size=2, hidden_size=4, lr=0.1, epochs=1000)\n",
        "mlp_xor.train(X_xor, y_xor)\n",
        "mlp_xor.evaluate(X_xor, y_xor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Code Explanation:**\n",
        "### **Code Explanation (Short Points)**  \n",
        "\n",
        "1. **Perceptron Class**  \n",
        "   - Implements a **single-layer perceptron** with a step activation function.  \n",
        "   - Uses **random weight initialization** (including bias).  \n",
        "   - Learning rate and epochs are configurable.  \n",
        "\n",
        "2. **Activation Function**  \n",
        "   - Uses a **threshold function**: returns `1` if input â‰¥ 0, otherwise `0`.  \n",
        "\n",
        "3. **Training Process**  \n",
        "   - Iterates through **epochs** and updates weights using the Perceptron Learning Rule\n",
        "   - Adds a **bias term** for better decision-making.  \n",
        "\n",
        "4. **Prediction Function**  \n",
        "   - Computes the **weighted sum** of inputs and applies the **activation function**.  \n",
        "\n",
        "5. **Evaluation with Confusion Matrix**  \n",
        "   - **Accuracy** is calculated as: (Correct Predictions/Total Samples)X 100\n",
        "   - **Confusion Matrix** is printed for detailed performance analysis.  \n",
        "\n",
        "7. **Limitations and Improvements**  \n",
        "   - **Single-layer perceptron** **cannot** solve **non-linearly separable** problems (like XOR).  \n",
        "   - **Solution:** Use a **multi-layer perceptron (MLP)** with **hidden layers** and **non-linear activations** (ReLU, Sigmoid)."
      ],
      "metadata": {
        "id": "NvYsnuiXOc73"
      },
      "id": "NvYsnuiXOc73"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**My Comments:**\n",
        "1. The step activation function is not ideal for backpropagation since it lacks smooth gradients.\n",
        "2. This MLP can learn XOR, but convergence is slower and less stable than using sigmoid or ReLU.\n",
        "3. The weight update could be improved with momentum or adaptive learning rates.\n",
        "4. Using sigmoid/ReLU activation would allow better gradient flow and training efficiency.\n",
        "5. The training process could benefit from batch updates rather than single-sample updates."
      ],
      "metadata": {
        "id": "ZsCkeUd5Ofay"
      },
      "id": "ZsCkeUd5Ofay"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7585c30-50d9-4c1e-af78-c07334909b84",
      "metadata": {
        "id": "c7585c30-50d9-4c1e-af78-c07334909b84"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
