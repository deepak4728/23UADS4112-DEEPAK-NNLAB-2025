{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Objective:**\n",
        "WAP to implement the Perceptron Learning Algorithm using numpy in Python. Evaluate performance of a single perceptron for NAND and XOR truth tables as input dataset."
      ],
      "metadata": {
        "id": "Ri3iJPEFnExl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Description of the Model:**\n",
        "- This is a single-layer perceptron model implemented using numpy.\n",
        "- It uses a step activation function (threshold function) and is trained using\n",
        "the perceptron learning rule. The model attempts to learn the NAND and XOR functions."
      ],
      "metadata": {
        "id": "SBovJiKlwJGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Python Implementation**"
      ],
      "metadata": {
        "id": "J6rizhYCMFji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, lr=0.1, epochs=100):\n",
        "        self.weights = np.random.randn(input_size + 1)  # Including bias\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def activation(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        x = np.insert(x, 0, 1)  # Adding bias term\n",
        "        return self.activation(np.dot(self.weights, x))\n",
        "\n",
        "    def train(self, X, y):\n",
        "        for _ in range(self.epochs):\n",
        "            for xi, target in zip(X, y):\n",
        "                xi = np.insert(xi, 0, 1)  # Adding bias term\n",
        "                pred = self.activation(np.dot(self.weights, xi))\n",
        "                error = target - pred\n",
        "                self.weights += self.lr * error * xi  # Update weights\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        predictions = [self.predict(x) for x in X]\n",
        "        correct = sum(p == t for p, t in zip(predictions, y))\n",
        "        accuracy = correct / len(y) * 100\n",
        "        print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(y, predictions)\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(cm)\n",
        "        return cm\n",
        "\n",
        "# NAND Truth Table\n",
        "X_nand = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_nand = np.array([1, 1, 1, 0])\n",
        "\n",
        "# XOR Truth Table\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Train Perceptron on NAND\n",
        "print(\"Training Perceptron for NAND\")\n",
        "nand_perceptron = Perceptron(input_size=2, lr=0.1, epochs=100)\n",
        "nand_perceptron.train(X_nand, y_nand)\n",
        "nand_perceptron.evaluate(X_nand, y_nand)\n",
        "\n",
        "# Train Perceptron on XOR\n",
        "print(\"\\nTraining Perceptron for XOR\")\n",
        "xor_perceptron = Perceptron(input_size=2, lr=0.1, epochs=100)\n",
        "xor_perceptron.train(X_xor, y_xor)\n",
        "xor_perceptron.evaluate(X_xor, y_xor)\n"
      ],
      "metadata": {
        "id": "NYudJVSkgHhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ec7cbf-fdd1-41a5-a007-341a2e32ec31"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perceptron for NAND\n",
            "Accuracy: 100.00%\n",
            "Confusion Matrix:\n",
            "[[1 0]\n",
            " [0 3]]\n",
            "\n",
            "Training Perceptron for XOR\n",
            "Accuracy: 50.00%\n",
            "Confusion Matrix:\n",
            "[[1 1]\n",
            " [1 1]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1],\n",
              "       [1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Code Explanation:**\n",
        "- `Perceptron class:` Implements the perceptron learning algorithm.\n",
        "- `train method:` Updates weights using the perceptron learning rule.\n",
        "- `predict method:` Uses the step activation function to classify inputs.\n",
        "- `evaluate method:` Computes accuracy."
      ],
      "metadata": {
        "id": "fmEOzoQQwVJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**My Comments:**\n",
        "1. The perceptron works well for NAND as it is a linearly separable problem.\n",
        "2. The model fails for XOR since it is not linearly separable, meaning a single-layer perceptron\n",
        "   cannot find a proper decision boundary.\n",
        "3. To learn XOR, a multi-layer perceptron (MLP) with a hidden layer and non-linear activation\n",
        "   (like sigmoid or ReLU) is needed.\n",
        "4. The training process could be improved by adding dynamic learning rates or momentum.\n",
        "5. The weight initialization could be refined to avoid slow convergence."
      ],
      "metadata": {
        "id": "80lE2fq4LH3r"
      }
    }
  ]
}
